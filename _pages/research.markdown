---
layout: default
title:  "Research"
permalink: /research/
---

## Current Research Interests



<!-- My current research spans simulation analysis and deep learning.  Incorporating HACC into the YT analysis framework enables simple Pythonic simulation analysis by non-experts, and allows quick iteration and validation by experts.  Now we can visualize stars and the galaxies identified in-situ:
![HACC galaxy](/assets/img/190203-core-mapped_Particle_z_particle_mass.png)

If we really want to showboat, we could also do a 3D projection using [napari](https://napari.org/stable) that includes gas density (blue), temperature (red-fuscia), and stars (white):
![HACC 3D](/assets/img/multichannel-stars_napari.png)


Aside from visualization and analysis work, prior work applying generative models to cosmology required the network model to generate the entire volume in one prediction.  We are working with novel AI accelerators such as SambaNova to enable volume generation of unprecedented scale.  You could find the first iteration of this work here: [Physical Benchmarking for AI-Generated Cosmic Web](https://arxiv.org/pdf/2112.05681.pdf).  The future of work in this project is to generate massive volumes at a rate to enable statistical studies of large numbers of cosmological realizations.

Finally, I am a lead researcher in developing and benchmarking methods of expanding context windows for large language models. In contrast to typical applications, we use LLMs to model genomic data, which is of a scale that LLMs typically do not handle.  For instance, a small book might require a context window of ~80-90K tokens, whereas the human genome has 3 *billion* base pairs.  Expanding the input window of LLMs would allow us to model and find relationships in genomic data at full-genome scale, enabling genomic-motivated treatment design, determining protein/gene interactions, or even modeling evolutionary mutations and thier effects. -->




